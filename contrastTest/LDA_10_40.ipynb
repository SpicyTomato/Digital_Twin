{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63cf063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "\n",
    "\n",
    "# 读取Excel文件\n",
    "file = '../file.xlsx'\n",
    "df = pd.read_excel(file, engine='openpyxl')\n",
    "\n",
    "# 数据预处理\n",
    "nltk.download('punkt')  # 新增：下载punkt资源\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str):  # 检查输入是否为字符串\n",
    "        return []  # 如果不是字符串，返回空列表\n",
    "\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "\n",
    "df['Processed_Abstract'] = df['Abstract'].apply(preprocess)\n",
    "\n",
    "# 构建文档-词汇矩阵\n",
    "dictionary = Dictionary(df['Processed_Abstract'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['Processed_Abstract']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,50,10)\n",
    "    # LDA建模\n",
    "    num_topics = i  # 选择主题数量\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)\n",
    "\n",
    "    # 可视化主题模型\n",
    "    vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.display(vis_data)\n",
    "    pyLDAvis.save_html(vis_data, 'lda_visualization.html')\n",
    "\n",
    "\n",
    "    from gensim.models import CoherenceModel\n",
    "\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=df['Processed_Abstract'], dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    print(f'Topic Coherence: {coherence_score}')\n",
    "\n",
    "    def get_topic_keywords(lda_model, top_n=10):\n",
    "        topic_keywords = []\n",
    "        for topic_id, topic_terms in lda_model.show_topics(num_topics=num_topics, num_words=top_n, formatted=False):\n",
    "            keywords = [term for term, _ in topic_terms]\n",
    "            topic_keywords.append(keywords)\n",
    "        return topic_keywords\n",
    "\n",
    "    topic_keywords = get_topic_keywords(lda_model, top_n=10)\n",
    "\n",
    "    from sklearn.metrics import jaccard_score\n",
    "\n",
    "\n",
    "    def topic_diversity(topic_keywords):\n",
    "        n_topics = len(topic_keywords)\n",
    "        jaccard_similarities = []\n",
    "\n",
    "        for i in range(n_topics):\n",
    "            for j in range(i + 1, n_topics):\n",
    "                jaccard_sim = jaccard_score(topic_keywords[i], topic_keywords[j], average='weighted')\n",
    "                jaccard_similarities.append(jaccard_sim)\n",
    "\n",
    "        diversity_score = 1 - sum(jaccard_similarities) / len(jaccard_similarities)\n",
    "        return diversity_score\n",
    "\n",
    "\n",
    "    diversity_score = topic_diversity(topic_keywords)\n",
    "    print(f'Topic Diversity: {diversity_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
